<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="longfangsong"><title>Region-Based Convolutional Networks for Accurate Object Detection and Segmentation翻译 · Blog</title><meta name="description" content="原文链接 https://ieeexplore.ieee.org/document/7112511/
    原作者Ross Girshick, Jeff Donahue, Student Member, IEEE,Trevor Darrell, Member, IEEE, and Jitendra"><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="apple-touch-icon-precomposed" sizes="144x144" href="/images/apple-touch-icon-144x144.png"><link rel="apple-touch-icon-precomposed" sizes="120x120" href="/images/apple-touch-icon-120x120.png"><link rel="apple-touch-icon-precomposed" sizes="72x72" href="/images/apple-touch-icon-72x72.png"><link rel="apple-touch-icon-precomposed" sizes="57x57" href="/images/apple-touch-icon-57x57.png"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/source-code-pro.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><meta name="generator" content="Hexo 4.2.1"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">Blog</a></h3><div class="description"><p>生命是灰色的，而理论之树常青</p></div></div><div id="pirate" data-wordart-src="//cdn.wordart.com/json/685czi4rqil5" style="width: 100%;" data-wordart-show-attribution></div><script src="//cdn.wordart.com/wordart.min.js" async defer></script></div><ul class="social-links"><li><a href="http://github.com/longfangsong" target="_blank" rel="noopener"><i class="fa fa-github"></i></a></li></ul><div class="footer"><a target="_blank" href="/"><span>Theme by </span></a><a href="https://www.caicai.me" target="_blank" rel="noopener"> CaiCai </a><span>&</span><a href="https://github.com/Ben02/hexo-theme-Anatole" target="_blank" rel="noopener"> Ben</a><div class="by_farbox"><a href="https://hexo.io/zh-cn/" target="_blank">Proudly published with Hexo&#65281;</a></div><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/tags">标签</a></li><li><a href="/archives">归档</a></li><li><a href="/links">友链</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img src="/images/logo.png"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h1><a>Region-Based Convolutional Networks for Accurate Object Detection and Segmentation翻译</a></h1></div><div class="post-content"><link rel="stylesheet noopener" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css" target="_blank"><h2 id="原文链接"><a class="headerlink" href="#原文链接" title="原文链接"></a>原文链接</h2><p> <a href="https://ieeexplore.ieee.org/document/7112511/" rel="noopener" target="_blank">https://ieeexplore.ieee.org/document/7112511/</a></p>
    <h2 id="原作者"><a class="headerlink" href="#原作者" title="原作者"></a>原作者</h2><p>Ross Girshick, Jeff Donahue, Student Member, IEEE,<br/>Trevor Darrell, Member, IEEE, and Jitendra Malik, Fellow, IEEE</p>
<h2 id="介绍"><a class="headerlink" href="#介绍" title="介绍"></a>介绍</h2><p>​        在图像中识别物体并将其定位是计算机视觉中最根本和最具挑战性的问题之一。在过去的十年中，这个问题已经取得了重大进展，主要是因为在复杂的机器学习框架中使用了SIFT和HOG等低级图像特征。但是，如果我们看一下典型视觉识别任务，PASCAL VOC对象检测，一般认为从2010年开始，其上的进展就变得缓慢了，只有一些通过构建整体系统并进行小修改而获得的微小进步。<br/>        SIFT和HOG是半局部直方图，这种表示我们可以粗略地其与V1，即灵长类视觉路径中的第一个皮层区域中的复杂细胞相关联。但是我们也知道几种下面几个皮层区域也进行了识别，这表明可能存在层次化，多阶段的计算特征，这些特征对于视觉识别更具信息性。<br/>        在这篇论文中，我们将介绍一个物体识别和分割系统，它使用了多层卷积神经网络来计算<del>高度区别但不变的</del>特征。在PASCAL测试中，这个系统与基于低级别图像特性的最佳方法相比，平均精度（mAP）上升了50％以上。我们的方法也可以与对象类别的数量保持一致，这对现有方法来说是一个长期存在的挑战。<br/>        我们的方法可以追根溯源到Fukushima的“neocognitron”方法，这是一种用于模式识别的分层和移位不变模型。虽然目前neocognitron的基本架构已经被广泛使用了，但Fukushima方法取得的成功有限，部分原因是它缺乏监督训练算法。 Rumelhart等人表明，类似的体系结构可以用监督的反向传播来训练，以对字符’T’和’C’的合成效果进行分类。在这项工作的基础上，LeCun等人展示了一个有影响力的论文序列来说明通过反向传播的随机梯度下降（SGD）对训练更深的网络以挑战真实世界的手写字符识别问题是有效的。这些模型现在称为卷积（神经）网络，CNN或ConvNets。</p>
<p>​        CNN在20世纪90年代被大量使用，但随着支持向量机的兴起而变得不再流行。 2012年，Krizhevsky等人，通过展示其在<em>Imagenet大规模视觉识别挑战（ILSVRC）</em>中准确性的大幅提升，重新激发了人们对CNN的兴趣。他们的成功源于使用120万张标签图像对一个大型CNN进行训练，以及20世纪90年代CNN上的一些进步（例如$max(x,0)$，“ReLU”非线性激活函数，“dropout”正则化以及快速的GPU实现）。</p>
<p>​        ILSVRC 2012研讨会期间，ImageNet结果的重要性引起了激烈的争论。核心问题可以归纳为以下几点：多大程度上，ImageNet上的CNN分类结果能够推广到PASCAL VOC挑战中的物体检测结果上？</p>
<p>​        我们在本论文的会议版本中回答了这个问题，表明与基于更简单HOG特征的系统相比，CNN可以在PASCAL VOC上获得更高的目标检测性能。为了达到这个结果，我们通过解决两个问题来解决图像分类和对象检测之间的差异：</p>
<ol>
<li>如何使用深度神经网络来定位物体？</li>
<li>如何用少量的有label的检测数据训练一个大容量的模型？</li>
</ol>
<p>​        与图像分类不同，检测需要在图像中定位（可能很多）对象。一种方法是将检测框架视为回归问题。这个例子可以很好地用于本地化单个对象，但导致多个对象需要复杂的解决方法或关于每个图像对象数量的特殊假设。另一种方法是构建一个滑动窗口检测器。 CNNs已经以这种方式使用了至少二十年，通常在受限物体类别上，例如人脸、手、和行人。这方面在计算效率方面很有吸引力，但其直接的应用需要所有对象都有一个共同的宽高比。宽高比问题可以用混合模型来解决，其中每个组件都专门研究<del>窄带宽高比或边界框回归</del>。<br/>        然而我们通过在“使用区域识别”方法来解决定位问题，这对于对象检测和语义分割来说都是成功的。在测试中，我们的方法为输入图像生成大约2000个与分类无关的区域数据，使用CNN从每组数据中提取固定长度的特征向量，然后使用特定类别的线性SVM对每个区域进行分类。我们使用简单的变形技术（各向异性图像缩放）来计算来自每个区域提案的固定尺寸的CNN输入，而不考虑区域的形状。图1显示了一个基于区域的卷积网络（R-CNN）的概况，并强调了我们的一些结果。<br/>        检测中面临的第二个挑战是标记数据稀少，目前可用的数量不足以从随机初始化中训练大型CNN。这个问题的传统解决方案是使用无监督的预训练，然后进行有监督的微调。本文的第二个主要贡献是显示对大型辅助数据集（ILSVRC）进行有监督的预训练，然后对小数据集（PASCAL）进行领域特定的微调，是数据稀少时训练大容量CNN有效范例。在我们的实验中，微调检测可以将mAP提高多达8个百分点。调整后，我们的系统对VOC 2010的分析结果为63％，而高度调整的基于HOG的可变形零件模型的分析结果为33％。<br/>        我们使用区域的最初动机源于实用主义的研究哲学：尽可能简单地从图像分类转向物体检测。从那时起，这种设计选择就被证明十分有价值，因为R-CNN的实现和训练都很直接（与滑动窗口CNN相比），并且它为对象检测和分割提供统一的解决方案。<br/>        这篇期刊论文以多种方式扩展了我们早期的工作。 首先，我们提供更多的实现细节，设计决策的理由和模型简化测试。 其次，我们使用更深的网络提供关于PASCAL检测的新结果。 我们的方法与所使用的网络体系结构的特定选择无关，并且我们显示最近关于更深层网络的工作对对象检测有巨大改进。 最后，我们将R-CNN与最近提出的OverFeat检测系统进行了头碰头的比较。 OverFeat使用滑动窗口CNN进行检测，是ILSVRC 2013检测挑战中性能最佳的方法。 我们训练的R-CNN显著优于OverFeat，在200-分类ILSVRC2013检测数据集中，我们的mAP为31.4％，而OverFeat为24.3％。</p>
<h2 id="相关工作"><a class="headerlink" href="#相关工作" title="相关工作"></a>相关工作</h2><h3 id="用于物体检测的深度CNN"><a class="headerlink" href="#用于物体检测的深度CNN" title="用于物体检测的深度CNN"></a>用于物体检测的深度CNN</h3><p>在R-CNN开发的同时，有一些其他的努力来使用卷积网络进行PASCAL风格的对象检测。 Szegedy等人的将对象检测建模为一个回归问题。给定一个图像窗口，他们使用CNN来预测整个对象的粗糙网格上的前景像素以及对象的顶部，底部，左侧和右侧。然后一个分组过程将找到的mask转换为检测到的边界框。 Szegedy等人从随机的初始值开始在VOC 2012上训练他们的模型，并在VOC 2007测试上获得30.7％的mAP。相比之下，使用相同网络架构的R-CNN获得58.5％的mAP，但使用了受监督的ImageNet预训练。一个假设是不使用ImageNet预训练使得表现更差。 Agrawal等人最近的工作表明情况并非如此；他们发现，<del>尽管只使用了一半数量的训练数据，随机初始化的，对VOC 2007训练的R-CNN</del>，在VOC 2007测试中达到了40.7％的mAP。</p>
<h3 id="可扩展性和速度"><a class="headerlink" href="#可扩展性和速度" title="可扩展性和速度"></a>可扩展性和速度</h3><p>除了精确之外，随着对象类别数量的增加，对象检测系统的可扩展性也很重要。在将诸如DPM之类的方法扩展到数以千计的对象类别的方面，人们已经做出了重大努力。例如，Dean等人用哈希表查找替换DPM中的提取过滤器卷积。他们表明，利用这种技术，可以在桌面工作站上以每个图像大约5分钟的时间运行10k DPM检测器。但是不幸地付出了一个代价：当大量DPM检测器竞争时，近似哈希方法会导致检测准确度大幅下降。相比之下，R-CNN可以随着要检测的对象分类的数量容易地扩展，因为几乎所有的计算都是在所有对象类别之间共享的。唯一的与分类有关的计算是一个相当小的矩阵-向量乘积和贪婪的非最大抑制。虽然这些计算与类别的数量成线性比例，但比例因子很小。根据经验测量，在CPU上检测200个类别比20个类别只多需要30毫秒，而没有任何近似。这使得在不对核心算法进行任何修改的情况下快速检测数以万计的对象类别变得可行。<br/>尽管有这种优雅的缩放行为，但根据所使用的网络，R-CNN在GPU上处理每个图像可能需要10到45秒，因为每个区域都是独立通过网络的。 He等人最近的工作（“SPPnet”）通过在特征金字塔上共享计算来提高R-CNN的效率，允许以每秒几帧的速度进行检测。 Girshick在SPPnet的基础上，表明通过使用称为“快速R-CNN”的方法，可以进一步缩短训练和测试时间，同时提高检测准确性并简化训练过程。根据网络架构不同，快速R-CNN能将检测时间（不包括区域提取计算）降低为每个图像50至300毫秒。</p>
<h3 id="定位方法"><a class="headerlink" href="#定位方法" title="定位方法"></a>定位方法</h3><p>目标检测的主要方法是基于滑动窗口检测器。这种方法至少可以追溯到早期人脸检测器，继续在使用基于HOG的行人检测和基于部分的通用对象检测。一个替代方案是首先计算一个（可能重叠的）图像区域池，每个图像区域都代表了一个候选物体，然后以旨在仅保留真实对象的方式过滤这些候选对象。 Hoiem等人使用了多重分割假设来估计粗糙的几何场景结构，Russell等人用这个方法来自动发现一组图像中的对象类。 van de Sande等人的“选择性搜索”算法。通过在PASCAL物体检测上显示出强大的结果推广了用于物体检测的多重分割方法。我们的方法受到了选择性搜索的启发。<br/>对象建议生成现在是一个活跃的研究领域。 EdgeBoxes可以快速输出高质量的矩形（方框）建议（每张图像约为0.3秒）。 BING以每张图像约3毫秒的速度生成方框建议，但随后发现这些建议质量太差，无法在R-CNN中使用。其他方法专注于逐像素分割，生成区域而不是框。这些方法包括RIGOR和MCG，每个图像需要10到30s，而GOP是一个更快的方法，每个图像需要花费约约1s。为了更深入地调查提议算法，Hosang等人为最近的方法提供了有洞察力的元评估。</p>
<h3 id="迁移学习"><a class="headerlink" href="#迁移学习" title="迁移学习"></a>迁移学习</h3><p>R-CNN的训练基于归纳<em>迁移学习</em>，使用Pan和Yang的分类。为了训练R-CNN，我们通常首先将ImageNet分类作为源任务和数据集，使用监督方法对网络进行训练，然后使用监督微调将该网络迁移到目标任务和数据集。该方法与传统的多任务学习相关，不同之处在于我们有序地进行任务训练，而且只关心网络在目标任务上是否表现出色。<br/>这种策略不同于最近的神经网络文献中<em>无监督转移学习</em>的主导范式。使用CNN监督转移学习，但不进行微调，也在Donahue等人的同时期的工作中进行了调查。 他们表明Krizhevsky等人的CNN，只要在ImageNet上进行过训练，就可以用作黑匣子特征提取器，在包括场景分类，细粒度子分类和领域适应在内的多项任务中表现出色。    Hoffman等人展示了如何使用迁移学习来使用具有图像级标签但没有bonding-box的训练数据来训练R-CNN。他们的方法基于对从图像分类到对象检测的任务转换进行建模，然后将该知识转换为没有检测训练数据的类。</p>
<h3 id="R-CNN的扩展"><a class="headerlink" href="#R-CNN的扩展" title="R-CNN的扩展"></a>R-CNN的扩展</h3><p>自推出以来，R-CNN已扩展到各种新任务和数据集。 Karpathy等人学习双向图像和句子检索的模型。他们的图像表示来源于R-CNN，该训练用于检测ILSVRC2013检测数据集上的200个类别。 Gkioxari等人使用多任务学习来训练R-CNN用于人员检测，2D姿态估计和动作识别。 Hariharan等人提出将目标检测和语义分割任务统一起来，称为“同时检测和分割”（SDS），并为此任务训练一个两列R-CNN。他们表明单个区域提取算法（MCG）可以有效用于传统的边界框检测以及语义分割。他们的PAS-CAL分割结果在本文报道的分割结果上显着改善。<br/>Gupta等人将R-CNN扩展到深度图中的物体检测。他们表明，精心设计的输入信号（其中深度图通过高于地面的高度以及相对于重力的局部表面方向来增强）允许训练超过现有RGB-D物体检测能力的R-CNN。 Song等人采用弱层次的图像层次监督训练R-CNN，采用子模块覆盖算法进行正面训练，然后训练潜在的SVM。</p>
<p>在最近的ILSVRC2014目标检测挑战中使用了许多基于或实现了R-CNN的系统，从而大大提高了检测精度。特别是获胜的方法GoogLeNet，它在R-CNN中使用了创新的网络设计。通过单个网络（以及稍微简化的，不包括SVM训练和边界框回归的管线），它们将R-CNN表现从34.5％的标准提高到38.0％mAP。他们还表明，六个网络的集合将其结果提高到43.9％的mAP。</p>
<h2 id="用R-CNN来进行物体识别"><a class="headerlink" href="#用R-CNN来进行物体识别" title="用R-CNN来进行物体识别"></a>用R-CNN来进行物体识别</h2><p>​        我们的物体检测系统由三个模块组成。第一个生成与类别无关的区域建议。这些建议定义了我们探测器可用的候选检测集。第二个模块是一个卷积网络，从每个区域提取一个固定长度的特征向量。第三个模块是一组特定类的线性SVM。 在本节中，我们将为每个模块提供我们的设计决策，描述他们的测试中使用情况，详细讲解如何训练他们的参数，并在PASCAL VOC 2010-12和ILSVRC2013上展示检测结果。</p>
<h3 id="模块设计"><a class="headerlink" href="#模块设计" title="模块设计"></a>模块设计</h3><h4 id="区域建议"><a class="headerlink" href="#区域建议" title="区域建议"></a>区域建议</h4><p>​        最近的各种论文提供了生成与分类无关的区域建议的方法。 例子包括：objectness，选择性搜索，类别无关的对象建议，约束参数最小切割（CPMC），多尺度组合分组和Ciresan。 虽然R-CNN对特定区域提议方法不可知，但我们使用选择性搜索来实现与先前检测工作的受控比较。</p>
<h3 id="特征提取"><a class="headerlink" href="#特征提取" title="特征提取"></a>特征提取</h3><p>​        我们用CNN从每个区域提取一个固定长度的特征向量。一般的CNN的架构是使用一个系统超参数。我们大部分的实验都使用由Krizhevsky介绍的CNN（TorontoNet），使用Caffe实现。然而我们也实验了Simonyan和Zisserman的16层深度网络（OxfordNet）。在每个情况下，特征向量都是4096维的。特征是通过向前传播一个减去平均数的 $S\times S$ RGB图像，并读取倒数第二层（soft-max分类器之前的层）输出的值。对于TorontoNet，$S = 227$，对于OxfordNet $S = 224$。<br/>        为了计算区域建议的特征，我们必须首先将该区域中的图像数据转换为与CNN兼容的格式（其架构需要固定$S\times S$像素大小的输入）。在许多可能的转换中，我们选择最简单的。无论区域大小或纵横比如何，我们都会将所有像素围绕在一个紧密的边界框中，以达到所需的大小。在变形之前，我们扩大紧密的边界框，以便在变形的大小时，在原始框的周围恰好有p像素在原来的图像上下文中（我们使用$p = 16$）。图2显示了对训练区域的随机抽样。 7.1节讨论了wraping的替代方法。</p>
<h3 id="测试时检测"><a class="headerlink" href="#测试时检测" title="测试时检测"></a>测试时检测</h3><p>​        在测试时，我们对测试图像进行选择性搜索，以提取大约2000个区域建议（在所有实验中，我们使用选择性搜索的“快速模式”）。我们wrap每个建议将其在CNN中正向传播，以计算特征。然后，对于每个类，我们使用支持该类的SVM对每个提取的特征向量进行评分。给定图像中的所有评分过的区域，我们应用一个贪婪的非最大抑制（对于每一个独立的分类），如果它与一个大于学习阈值的更高的得分选择区域有一个重叠的交叉点（IOU），则拒绝这个区域。</p>
<h4 id="运行时分析"><a class="headerlink" href="#运行时分析" title="运行时分析"></a>运行时分析</h4><p>​        两个特性使得检测有效。首先，所有的CNN参数会在所有类别中共享。第二，与其他常用方法相比，CNN计算的特征矢量是低维的，例如具有视觉字编码的空间金字塔。例如，在UVA检测系统中使用的特征是比我们的大两个数量级（360 k比4 k维）。</p>
<p>​        这种共享的结果是，在所有的类上，花费计算机时间的建议和特征（在Nvidia Titan黑色GPU上速度为10s/图像，在CPU上是53s/图像，使TorontoNet）被摊销。唯一的类具体计算是特征和SVM权重和非最大抑制之间的点积。在实践中，图像的所有点积被分成单个矩阵矩阵乘积。特征矩阵通常是2000×4096，SVM权重矩阵是4096N，其中N是类的数目。</p>
<p>​        该分析表明，R-CNNs可以扩展到数千个对象类，而不诉诸近似技术，例如哈希。即使有100k个类，结果矩阵的乘法在现代多核CPU上只需10秒。这种效率不仅仅是使用区域建议和共享特征的结果。由于其高维度的特征，UVA系统将慢两个数量级，而且需要134 GB的存储器来存储100 k线性预测因子，而对于我们的低维特征而言，仅需要1.5 GB。<br/>        另外比较有趣的是，R-CNNs与最近的Dean等人的工作进行对比。他们报告说基于DPMS和哈希的可伸缩检测在VOC 2007上有约16%的mAP，在10 k个分类的情况下。用我们的方法，10 k检测器可以在CPU上运行大约一分钟，并且因为没有近似，mAP将保持在59%（TorontoNet）和66%（OxfordNet）。</p>
<h3 id="训练"><a class="headerlink" href="#训练" title="训练"></a>训练</h3><h4 id="受监督的预训练"><a class="headerlink" href="#受监督的预训练" title="受监督的预训练"></a>受监督的预训练</h4><p>我们仅使用图像级注释（此数据集没有边界框标签）在大型辅助数据集（ILSVRC2012分类）上有区别地预先训练了CNN。 使用开源的Caffe CNN库进行预训练。</p>
<h4 id="特定领域的微调"><a class="headerlink" href="#特定领域的微调" title="特定领域的微调"></a>特定领域的微调</h4><p>为了使CNN适应新的任务（检测）和新的领域（wrap过的建议窗口），我们仅使用wrap过的区域建议继续对CNN参数进行随机梯度下降训练。除了用随机初始化的$(N+1)$-分类层（其中N是对象类的数量，加上1作为背景）替换CNN的特定于ImageNet的1000路分类层之外，CNN体系结构不变。对于VOC，$N=20$，对于ILSVRC2013，$N = 200$。我们将所有与确定的正面样本的包围盒重叠IoU$\ge 0.5$的区域作为正面结果，其余为负面结果。我们以0.001的学习率（初始预训练比率的1/10）开始SGD，这使得微调可以取得进展，而不会破坏初始化。在每次SGD迭代中，我们统一采样32个窗口（覆盖所有类别）和96个背景窗口，构建128个mini-batch。我们将采样偏向正窗口，因为它们与背景相比极为罕见。 OxfordNet需要比TorontoNet更多的内存，因此有必要减小minibatch的大小以适应单个GPU。我们将批量从128个减少到了24个，同时保持了相同的偏置采样方案。</p>
<h4 id="对象分类器"><a class="headerlink" href="#对象分类器" title="对象分类器"></a>对象分类器</h4><p>​        考虑训练一个二元分类器来检测汽车。很显然，一个紧紧围绕汽车的图像区域应该是一个正样本。同样，很显然，与汽车无关的背景区域应该是一个负样本。不那么显而易见的是如何标记与汽车部分重叠的区域。我们用一个IoU重叠阈值来解决这个问题，低于这个阈值的区域被定义为负样本。重叠阈值，$0.3$，是通过一个验证集上的，从$\{0,0.1,…,0.5\}$的网格搜索中来选择的。我们发现仔细选择这个阈值是很重要的。将其设置为$0.5$，将会使mAP降低5个点。同样，将其设置为0，将使mAP降低四个点。正面的例子被简单地定义为每个类的绝对边界框。<br/>        一旦提取了特征并应用了训练标签，我们就为每个类优化一个线性SVM。由于训练数据太大而不适合记忆，我们采用标准的硬性负挖掘法。硬性负挖掘迅速收敛，实际上，在所有图像上只经过一次扫描后，mAP就会停止增加。<br/>        在第7.2节中，我们讨论了为什么在微调和SVM训练中正反例子的定义不同。我们还讨论了训练检测SVM所涉及的权衡问题，而不是简单地使用来自微调CNN的最终softmax层的输出。</p>
<h3 id="PASCAL-VOC-2010-12上的结果"><a class="headerlink" href="#PASCAL-VOC-2010-12上的结果" title="PASCAL VOC 2010-12上的结果"></a>PASCAL VOC 2010-12上的结果</h3><p>​        遵循PASCAL VOC最佳实践，我们在VOC 2007数据集中验证了所有设计决策和超参数。为了在VOC 2010-12数据集上获得最终结果，我们对针对VOC 2012进行了CNN的微调，并优化了我们的检测SVM。我们仅向评估服务器提交了两个主要算法变体（带和不带边界框回归）的测试结果各一次。<br/>        表1显示了VOC 2010的完整结果。我们将我们的方法与包括SegDPM，一个将DPM检测器与语义分割系统的输出相结合，并使用额外的内部检测器上下文和图像分类器重新分类的算法在内的四个强基准进行了比较。与Uijlings等人的UVA系统比较最密切，因为我们的系统使用相同的区域提议算法。为了对区域进行分类，他们的方法建立了一个四级空间金字塔，并使用密集取样的SIFT，Extended OpponentSIFT和RGB-SIFT描述符来填充它们，每个矢量用4,000字的码字进行量化。分类是用直方图交叉核支持向量机进行的。与其多特征非线性核SVM方法相比，我们的mAP的大幅提高，其中TorontoNet的mAP从35.1％提升至53.7％，OxfordNet提升为62.9％，同时速度也快得多。 R-CNN在VOC 2012测试中表现相似（53.3％/62.4％mAP）。</p>
<h3 id="ILSVRC2013检测上的结果"><a class="headerlink" href="#ILSVRC2013检测上的结果" title="ILSVRC2013检测上的结果"></a>ILSVRC2013检测上的结果</h3><p>​       我们使用与PASCAL VOC相同的系统超参数在200级ILSVRC2013检测数据集上运行R-CNN。我们遵循了相同的将测试结果提交给ILSVRC2013评估服务器的方式，即两次，一次和一次没有边界框回归。<br/>图3比较了我们的R-CNN在ILSVRC 2013比赛中和前冠军OverFeat比赛的结果。使用TorontoNet，我们的R-CNN达到了31.4％的平均分，领先于OverFeat的24.3％的第二好成绩。为了理解AP在各个分类上的分布情况，还列出了盒子图。大多数竞争者（OverFeat，NEC-MU，Toronto A和UIUC-IFP）都使用卷积网络，这表明CNN如何应用于对象检测具有重要的差别，导致结果大不相同。值得注意的是，UvA-Euvision的入口并没有使用CNN，而是基于快速的VLAD编码。</p>
<p>​        在第5节中，我们概述了ILSVRC2013检测数据集，并提供了关于在训练R-CNN时做出的选择的详细信息。</p>
<h2 id="分析"><a class="headerlink" href="#分析" title="分析"></a>分析</h2><h3 id="可视化学到的特征"><a class="headerlink" href="#可视化学到的特征" title="可视化学到的特征"></a>可视化学到的特征</h3><p>​        第一层过滤器可以直接可视化并且易于理解。他们提取了面向边缘和相对的的颜色。了解后续的图层更具挑战性。 Zeiler和Fergus提出了一种视觉吸引力的反卷积方法。我们提出一个简单的（和完备的）非参数方法，直接显示网络学到了什么。<br/>这个想法是在网络中挑出一个特定的单元（特征），并将它用作它本身就是一个物体检测器。也就是说，我们计算单位在一系列大型推荐区域提议（大约1000万）上的激活情况，将提案从最高激活到最低激活进行排序，执行非最大抑制，然后显示评分最高的区域。我们的方法通过精确地显示了它所触发的输入，让选定的单位“自我展示”。为了看到不同的视觉模式并且深入了解单元计算的不变性，我们避免求平均值。<br/>        我们可视化来自TorontoNet的$pool_5$层的单元，这是网络第五层和最后一个卷积层的最大混合输出。 $pool_5$特征图是$6\times 6\times 256 = 9216$维。忽略边界效应，每个$pool_5$单位有一个$195 \times 195$的接受区域在原来的$227\times 227$输入。一个中央的$pool_5$单位几乎具有全局视野，而靠近边缘的则拥有较小的，剪裁过的支撑。</p>
<p>图4中的每一行显示了我们在VOC 2007 trainval上进行了微调的来自CNN的$pool_5$单元的前16个激活。 256个功能独特的装置中有6个可视化。选择这些单位是为了显示网络学习内容的代表性样本。在第二行中，我们看到一个单位在狗脸和点阵上激活。对应于第三行的单位是红色斑点检测器。还有用于人脸和更抽象的图案的检测器，例如带有窗口的文本和三角形结构。该网络似乎学习了一种表示法，它将少量的类调整功能与形状，纹理，颜色和材料属性的分布式表示结合在一起。随后的完全连接层$fc6$能够模拟这些丰富特征的大量组合。 Agrawal等人提供对学习到的特征更深入的分析。</p>
<h3 id="简化测试"><a class="headerlink" href="#简化测试" title="简化测试"></a>简化测试</h3><p>（完全不明白，略）</p>
<h3 id="网络架构"><a class="headerlink" href="#网络架构" title="网络架构"></a>网络架构</h3><p>​        这篇论文中的大部分结果都是使用的Krizhevsky等人的TorontoNet，然而，我们发现架构的选择对R-CNN检测的性能有很大的效果。</p>
<p>​        在表3中，我们使用Simonyan和Zisserman最近提出的16层深的OxfordNet显示了VOC 2007测试的结果。 该网络是最近ILSVRC 2014分类挑战中表现最佳的之一。 该网络具有由13层$3\times 3$卷积核，其中有5个max池化层，并且有三个全连接层。</p>
<p>​        为了在R-CNN中使用OxfordNet，我们从Caffe模型动物园下载了公开可用的<code>VGG_ILSVRC_16_layers</code>模型的预先训练好的网络权重。然后，我们使用与TorontoNet相同的方法对网络进行了微调。 唯一的区别是根据需要使用更小的minibatches（24个示例）以适应显存。 表3中的结果表明，使用OxfordNet的R-CNN与TorontoNet相比大大优于R-CNN，将mAP从58.5％提高到66.0％。 然而，在计算时间方面存在相当大的缺陷，OxfordNet的正向传播时间比TorontoNet长七倍。</p>
<p>​        从迁移学习的角度来看，图像分类的巨大改进直接转化为对象检测的巨大改进非常令人鼓舞。</p>
<h3 id="检测错误分析"><a class="headerlink" href="#检测错误分析" title="检测错误分析"></a>检测错误分析</h3><p>​        我们应用了Hoiem等人的优秀检测分析工具。 为了显示我们的方法的错误中的模式，了解微调如何改变它们，并了解我们的错误类型与DPM的比较。 分析工具的完整摘要超出了本文的范围，我们鼓励读者阅读参考以了解一些更精细的细节（例如“规范化的AP”）。 由于分析最好在关联图的背景下进行，因此我们将讨论放在图5和图6的标题内。</p>
<h3 id="边界框回归"><a class="headerlink" href="#边界框回归" title="边界框回归"></a>边界框回归</h3><p>​        基于错误分析，我们实施了一种简化方法来减少定位错误。 受到DPM中使用的边界框回归的启发，我们训练线性回归模型以预测给定选择性搜索区域提案的$pool_5$特征的新检测窗口。 详细信息在第7.3节中给出。 表1,2和5中的结果表明，这种简单的方法可以修复大量错误定位检测，将mAP提高3到4个点。</p>
<h3 id="定性结果"><a class="headerlink" href="#定性结果" title="定性结果"></a>定性结果</h3><p>​        ILSVRC2013的定性检测结果如图8所示。每个图像从$val_2$组中随机采样，精度大于0.5的所有检测器的所有检测结果都被显示。 <del>请注意，这些没有策划，并给出了实际检测器的实际印象。</del></p>
<h2 id="ILSVRC2013检测数据集"><a class="headerlink" href="#ILSVRC2013检测数据集" title="ILSVRC2013检测数据集"></a>ILSVRC2013检测数据集</h2><p>在第3节中，我们介绍了ILSVRC2013检测数据集的结果。 该数据集与PASCAL VOC不太一样，因此需要选择如何使用它。 由于这些决定并非微不足道，所以我们在本节中介绍它们。 本节介绍的方法学和“$val_1$”和“$val_2$”数据拆分被ILSVRC2014检测挑战的参与者广泛使用。</p>
<p>（不太重要，略）</p>
<h2 id="实现了设计细节"><a class="headerlink" href="#实现了设计细节" title="实现了设计细节"></a>实现了设计细节</h2><h3 id="对象建议转换"><a class="headerlink" href="#对象建议转换" title="对象建议转换"></a>对象建议转换</h3><p>在这项工作中使用的卷积网络需要固定尺寸的输入（例如$227 \times 227$像素）以产生固定尺寸的输出。 为了检测，我们考虑任意图像矩形的对象提议。 我们评估了将对象提议转换为有效的CNN输入的两种方法。</p>
<p>​        第一种方法（“带有上下文的最紧密方形”）将每个对象提议包含在最紧密的正方形内，然后将包含在该正方形中的图像缩放（各向同性）为CNN输入大小。图7栏（B）显示了这种转变。这种方法的一个变体（“不带上下文的最紧密方形”）排除了围绕原始对象提议的图像内容。图7栏（C）显示了这个转换。第二种方法（“warp”）将每个对象提议各向异性地缩放到CNN输入大小。图7栏（D）显示了warp变换的结果。<br/>        对于这些转换中的每一个，我们还考虑在原始对象提议中包含附加的图像上下文。上下文填充量（$p$）被定义为围绕原始对象提议在经过转换的输入坐标框中的边框大小。图7显示了每个实例的顶行中的$p = 0$个像素以及底行中的$p = 16$个像素。在所有方法中，如果源矩形延伸超出图像，丢失的数据将被图像平均值替换（然后在将图像输入到CNN之前将其减去）。一组试验表明，使用上下文填充（$p = 16$像素）的wrap比较大幅度地（3-5 mAP）优于其他方案。显然，更多的选择是可能的，包括使用图像的复制而不是平均值填充。对这些选项进行彻底评估是未来的工作。</p>
<h3 id="正例、负例和Softmax"><a class="headerlink" href="#正例、负例和Softmax" title="正例、负例和Softmax"></a>正例、负例和Softmax</h3><p>​        两个设计选择值得进一步讨论。首先是：为了微调CNN与训练物体检测SVM，为什么正面和负面的例子有不同的定义？为了简要回顾这些定义，为了进行微调，我们将每个对象提议映射到最大IoU重叠（如果有的话）的确定正例，如果IoU大于0.5，就将其标记为匹配确定正例的正例。所有其他提案都被标记为“背景”（即所有其他类别的反例）。相比之下，对于训练支持向量机，相比之下，我们仅将确定正例作为其相应类别的正面例子，并将少于0.3个IoU的标签提案与所有类别的实例重叠作为该类别的否定类别。落入灰色区域的提案（超过0.3个IoU重叠，但不是确定正例）将被忽略。<br/>        从历史的角度来说，我们已经达到了这些定义，因为我们开始通过对由ImageNet预先训练的CNN计算出的特征训练SVM，所以微调并不是当时的考虑因素。在该设置中，我们发现我们用于训练SVM的特定标签定义在我们评估的一组选项（其中包括我们现在用于微调的设置）内是最优的。当我们开始使用微调时，我们最初使用与我们用于SVM训练相同的正和负示例定义。然而，我们发现结果比使用我们目前的正面和负面定义获得的结果差得多。<br/>        我们的假设是，正负如何定义的差异并不是根本重要的，并且源于微调数据有限的事实。我们目前的计划引入了许多“抖动”的例子（那些重叠度在0.5和1之间的提案，但不包括基本事实），它将正面实例的数量扩大了大约30倍。我们猜想，在微调整个网络以避免过度拟合时，需要这个大集合。但是，我们也注意到，使用这些抖动示例可能不是最理想的，因为网络未精确调整以实现精确定位。<br/>        这导致了第二个问题：为什么要使用SVM？简单地将微调网络的最后一层应用为对象检测器，这是一种21路soft-max分类器，它会更清晰。我们尝试过这一点，发现VOC 2007的性能从54.2降至50.9％。这种性能下降可能来自多个因素的综合作用，包括用于微调的正面示例的定义不强调精确定位，并且softmax分类器是随机采样的负面示例而不是用于SVM训练中的“确定负例”。<br/>        这一结果表明，在微调后无需训练SVM就可以获得接近相同性能水平的结果。我们猜测，通过一些额外的调整来微调，剩余的效果差距可能会被减小。如果属实，这将简化并加速R-CNN训练，同时不会降低检测性能。</p>
<h3 id="边界框回归-1"><a class="headerlink" href="#边界框回归-1" title="边界框回归"></a>边界框回归</h3><p>​        我们使用简单的边界框回归阶段来提高定位性能。 在使用特定于类别的检测SVM对每个选择性搜索提议评分之后，我们使用特定于类别的边界框回归器预测用于检测的新边界框。 这与可变形零件模型中使用的边界框回归相似。 这两种方法之间的主要区别在于，这里我们从CNN计算的特征回归，而不是从推断的DPM零件位置计算的几何特征。</p>
<p>​        我们的训练算法的输入是一组$N$个的训练对$\{P^i,G^i\}_i=1,…,N$，其中$P^i=(P^i_x,P^i_y,P^i_w,P^i_h)$，定义了Pi的边界框的中心的像素坐标以及以像素为单位的Pi的宽度和高度。 为简便考虑，除非需要，否则我们将不写出上标$i$。 每个确定正例边界框$G$都以相同的方式指定：$G^i=(G^i_x,G^i_y,G^i_w,G^i_h)$。我们的目标是学习一个转换，能将提议框$P$转换到确定正例边界框$G$。</p>
<p>​        我们将转换的系数设为四个函数$d_x(P)$，$d_y(P)$，$d_w(P)$，$d_h(P)$，前两个确定了一个关于$P$边界框中心的尺度无关转换，接下来的两个确定了$P$边界框宽度和高度在log空间上的转换。在学习了这些函数之后，我们可以通过使用下面的变换将输入的提议$P$转化为一个确定正例的边界框$\hat G$：</p>
<script type="math/tex; mode=display">
\hat G_x = P_w d_x(P) + P_x \\
\hat G_y = P_h d_y(P) + P_y \\
\hat G_w = P_w exp(d_w(P)) \\
\hat G_h = P_h exp(d_h(P))</script><p>​        每个函数$d_\star (P)$（其中$\star$是$x,y,h,w$中的一个）都是对建议$P$在$pool_5$中的特征进行建模的一个线性函数，由$\phi_5(P)$表示。（隐式地假设$\phi_5(P)$对图像数据的依赖性）。从而我们有$d\star(P)=w_\star^T\phi_5(P)$，其中$w_\star$是一个可以被学习的模型参数向量。我们通过最小化正规化的方差来学习学习$w_\star$：</p>
<script type="math/tex; mode=display">
\textbf w_\star=\arg \min \limits_{\hat {\textbf  w}_\star} \sum_i^n(t^i_\star-\hat {\textbf  w}_\star^T\phi_5(P^i))^2+\lambda||\hat {\textbf  w}_\star||^2</script><p>​        训练对$(P,G)$的回归目标$t_\star$被定义为：</p>
<script type="math/tex; mode=display">
t_x=(G_x-P_x)/P_w \\
t_y=(G_y-P_y)/P_h \\
t_w=log(G_w-P_w) \\
t_h=log(G_h/P_w)</script><p>​        作为一个标准的正则化最小二乘问题，这可以通过封闭形式有效地解决。<br/>        我们在实现边界框回归时发现了两个微妙的问题。首先是正则化非常重要：我们在验证集上设置$\lambda = 1000$。第二个问题是在选择使用哪些训练对$(P,G)$时必须小心。直观地说，如果P远离所有的确定正例框，那么将$P$转换为确定正例框$G$的任务就没有意义了。使用像$P$这样的例子会导致无望的学习问题。因此，我们只从在一个确定正例框附近的建议P中学习。当且仅当重叠大于阈值（在验证集上我们将其设置为0.6）时，我们才通过将$P$分配给与其具有最大IoU重叠（如果它与不止一个重叠）的确定正例框$G$来实现“接近度”。所有未分配的提议都将被丢弃。我们为每个对象类执行一次这样的操作，以便学习一组特定于类的边界框回归器。<br/>在测试时，我们对每个提议进行评分，并预测其新的检测窗口一次。原则上，我们可以迭代这个过程（即，重新评分新预测的边界框，然后从中预测新的边界框，等等）。但是，我们发现迭代不会改进结果。</p>
<h3 id="跨数据集冗余的分析"><a class="headerlink" href="#跨数据集冗余的分析" title="跨数据集冗余的分析"></a>跨数据集冗余的分析</h3><p>​        在对辅助数据集进行训练时需要考虑的一点是，它和测试集之间可能存在冗余。尽管目标检测和全图像分类的任务有很大不同，但使得这种交叉冗余更加不那么令人担忧，我们仍然进行了一次彻底调查，以量化ILSVRC 2012培训中包含的PASCAL测试图像的范围和验证集。我们的发现对于有兴趣使用ILSVRC 2012作为PASCAL图像分类任务的训练数据的研究人员可能会有用。<br/>我们对重复（和接近重复）的图像执行了两次检查。第一个测试基于flickr图像ID的精确匹配，这包含在VOC 2007测试注释中（对于后续的PASCAL测试集，这些ID是有意保密的）。所有PASCAL图像，大约一半的ILSVRC，都是从flickr.com收集的。这项检查发现了4952个（0.63％）中的31个。<br/>         第二次检查使用GIST描述符匹配，这个方法在大型（&gt; 100万）图像集合中接近重复图像检测时具有出色的性能。接下来，我们计算了所有ILSVRC 2012 train-val和PASCAL 2007测试图像的wrap过的32 * 32像素版本上的GIST描述符。<br/>        GIST描述符的欧几里得距离最近邻居匹配揭示了38个近似重复的图像（包括通过flickr ID匹配找到的全部31个）。这些匹配在JPEG压缩级别和分辨率方面往往略有不同，并且在较小程度上裁剪。这些研究结果表明，重叠很小，不到1％。对于VOC 2012，由于flickr ID不可用，我们只使用GIST匹配方法。根据GIST匹配，VOC 2012测试图像中有1.5％在ILSVRC 2012训练中。 2012年VOC的数据略高可能是由于两个数据集的收集时间比VOC 2007和ILSVRC 2012更接近。</p>
<h2 id="结论"><a class="headerlink" href="#结论" title="结论"></a>结论</h2><p>​        近年来，物体检测性能停滞不前。性能最好的系统是将多个低级图像特征与来自物体检测器和场景分类器的高级上下文相结合的复合体。本论文介绍了一种简单且可扩展的物体检测算法，与PASCAL VOC 2012上的最佳结果相比，其性能提高了50％以上。<br/>        我们通过两个见解实现了这一表现。首先是将大容量卷积网络应用于自下而上的区域提案，以便对对象进行定位和分段。第二种是训练数据稀缺时训练大型CNNs的范例。我们表明，对网络进行预训练（监管）对于具有丰富数据的辅助任务（图像分类）非常有效，然后针对数据稀缺的目标任务（检测）对网络进行微调。我们推测“监督式预训练/领域特定微调”范式对于各种数据稀缺的视觉问题将非常有效。<br/>        最后我们得出结论，我们通过结合使用计算机视觉和深度学习的经典工具（自下而上区域提议和卷积网络）来实现这些结果是非常重要的。这两者不是无关的科学探究，而是自然而不可避免的合作伙伴。</p>
<p>​    </p>

</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2018-04-23</span><i class="fa fa-tag"></i><a class="tag" href="/tags/机器视觉/" title="机器视觉">机器视觉 </a><a class="tag" href="/tags/AI/" title="AI">AI </a><a class="tag" href="/tags/论文翻译/" title="论文翻译">论文翻译 </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,http://longfangsong.github.io/2018/04/22/Region-Based Convolutional Networks for Accurate Object Detection and Segmentation翻译/,Blog,Region-Based Convolutional Networks for Accurate Object Detection and Segmentation翻译,;" target="_blank" rel="noopener"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2018/04/24/Linux%E5%91%BD%E4%BB%A4cheatsheet/" title="Linux命令cheatsheet">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2018/04/18/Rapid%20Object%20Detection%20Using%20a%20Boosted%20Cascade%20of%20Simple%20Features%E7%BF%BB%E8%AF%91/" title="Rapid Object Detection Using a Boosted Cascade of Simple Features翻译">下一篇</a></li></ul></div><a id="comments"></a><div id="vcomments" style="margin:0 30px;"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/gh/xcss/valine@v1.1.7/dist/Valine.min.js?v=undefined"></script><script>var valine = new Valine({
  el:'#vcomments',
  notify:false || false, 
  verify:false|| false, 
  app_id:'HcCTt2sXmRChcaE4nMoSibwJ-gzGzoHsz',
  app_key:'6eevqWRr7jsWPUavkkAQjIMi',
  placeholder:'随便说点啥吧……',
  path: window.location.pathname,
  avatar:'mm'
})</script><script src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ["\\(","\\)"]],
    displayMath: [['$$','$$'], ["\\[","\\]"]]
  }
});</script></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>